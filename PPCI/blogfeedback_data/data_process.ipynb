{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2b799fd-da9c-41a8-9f24-c34b37a59b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in zip: ['blogData_test-2012.02.01.00_00.csv', 'blogData_test-2012.02.02.00_00.csv', 'blogData_test-2012.02.03.00_00.csv', 'blogData_test-2012.02.04.00_00.csv', 'blogData_test-2012.02.05.00_00.csv', 'blogData_test-2012.02.06.00_00.csv', 'blogData_test-2012.02.07.00_00.csv', 'blogData_test-2012.02.08.00_00.csv', 'blogData_test-2012.02.09.00_00.csv', 'blogData_test-2012.02.10.00_00.csv', 'blogData_test-2012.02.11.00_00.csv', 'blogData_test-2012.02.12.00_00.csv', 'blogData_test-2012.02.13.00_00.csv', 'blogData_test-2012.02.14.00_00.csv', 'blogData_test-2012.02.15.00_00.csv', 'blogData_test-2012.02.16.00_00.csv', 'blogData_test-2012.02.17.00_00.csv', 'blogData_test-2012.02.18.00_00.csv', 'blogData_test-2012.02.19.00_00.csv', 'blogData_test-2012.02.20.00_00.csv', 'blogData_test-2012.02.21.00_00.csv', 'blogData_test-2012.02.22.00_00.csv', 'blogData_test-2012.02.23.00_00.csv', 'blogData_test-2012.02.24.00_00.csv', 'blogData_test-2012.02.25.00_00.csv', 'blogData_test-2012.02.26.00_00.csv', 'blogData_test-2012.02.27.00_00.csv', 'blogData_test-2012.02.28.00_00.csv', 'blogData_test-2012.02.29.00_00.csv', 'blogData_test-2012.03.01.00_00.csv', 'blogData_test-2012.03.02.00_00.csv', 'blogData_test-2012.03.03.00_00.csv', 'blogData_test-2012.03.04.00_00.csv', 'blogData_test-2012.03.05.00_00.csv', 'blogData_test-2012.03.06.00_00.csv', 'blogData_test-2012.03.07.00_00.csv', 'blogData_test-2012.03.08.00_00.csv', 'blogData_test-2012.03.09.00_00.csv', 'blogData_test-2012.03.10.00_00.csv', 'blogData_test-2012.03.11.00_00.csv', 'blogData_test-2012.03.12.00_00.csv', 'blogData_test-2012.03.13.00_00.csv', 'blogData_test-2012.03.14.00_00.csv', 'blogData_test-2012.03.15.00_00.csv', 'blogData_test-2012.03.16.00_00.csv', 'blogData_test-2012.03.17.00_00.csv', 'blogData_test-2012.03.18.00_00.csv', 'blogData_test-2012.03.19.00_00.csv', 'blogData_test-2012.03.20.00_00.csv', 'blogData_test-2012.03.21.00_00.csv', 'blogData_test-2012.03.22.00_00.csv', 'blogData_test-2012.03.23.00_00.csv', 'blogData_test-2012.03.24.00_00.csv', 'blogData_test-2012.03.25.00_00.csv', 'blogData_test-2012.03.26.01_00.csv', 'blogData_test-2012.03.27.01_00.csv', 'blogData_test-2012.03.28.01_00.csv', 'blogData_test-2012.03.29.01_00.csv', 'blogData_test-2012.03.30.01_00.csv', 'blogData_test-2012.03.31.01_00.csv', 'blogData_train.csv']\n",
      "原始 df_raw 形状: (52397, 281)\n",
      "去掉整行重复后行数: 49203\n",
      "最终去掉 X 重复后行数: 48439\n",
      "X_clean_raw shape: (48439, 280)\n",
      "Y_clean_raw shape: (48439,)\n",
      "Y_clean_raw min/max: 0.0 1370.0\n",
      "标准化后 X_clean_std 形状: (48439, 280)\n",
      "用于后续 train/test split 的剩余样本数: 48389\n",
      "LightGBM 额外训练样本数: 33872\n",
      "PPCI 样本数:             14517\n",
      "LightGBM 总训练样本数 (含 x0): 33922\n",
      "Fitting LightGBM ...\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 1.892884 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8503\n",
      "[LightGBM] [Info] Number of data points in the train set: 27137, number of used features: 212\n",
      "[LightGBM] [Info] Start training from score 0.608789\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 157\u001b[0m\n\u001b[1;32m    149\u001b[0m X_tr, X_val, Y_tr, Y_val \u001b[38;5;241m=\u001b[39m train_test_split(\n\u001b[1;32m    150\u001b[0m     X_pred,\n\u001b[1;32m    151\u001b[0m     Y_pred_log,\n\u001b[1;32m    152\u001b[0m     test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m,\n\u001b[1;32m    153\u001b[0m     random_state\u001b[38;5;241m=\u001b[39mRANDOM_STATE,\n\u001b[1;32m    154\u001b[0m )\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting LightGBM ...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 157\u001b[0m \u001b[43mlgbm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_tr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m    \u001b[49m\u001b[43mY_tr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43ml2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;66;03m# 在 PPCI 那部分上预测（log 标度）\u001b[39;00m\n\u001b[1;32m    165\u001b[0m Yhat_ppci_log \u001b[38;5;241m=\u001b[39m lgbm\u001b[38;5;241m.\u001b[39mpredict(X_ppci)\n",
      "File \u001b[0;32m~/yangsuienv/lib/python3.10/site-packages/lightgbm/sklearn.py:1398\u001b[0m, in \u001b[0;36mLGBMRegressor.fit\u001b[0;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_init_score, eval_metric, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[1;32m   1381\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(  \u001b[38;5;66;03m# type: ignore[override]\u001b[39;00m\n\u001b[1;32m   1382\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1383\u001b[0m     X: _LGBM_ScikitMatrixLike,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1395\u001b[0m     init_model: Optional[Union[\u001b[38;5;28mstr\u001b[39m, Path, Booster, LGBMModel]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1396\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLGBMRegressor\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1397\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Docstring is inherited from the LGBMModel.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1398\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1399\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1400\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1401\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1402\u001b[0m \u001b[43m        \u001b[49m\u001b[43minit_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1403\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1404\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1405\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_sample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1406\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_init_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_init_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1407\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_metric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeature_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcategorical_feature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcategorical_feature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1411\u001b[0m \u001b[43m        \u001b[49m\u001b[43minit_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1412\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1413\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/yangsuienv/lib/python3.10/site-packages/lightgbm/sklearn.py:1049\u001b[0m, in \u001b[0;36mLGBMModel.fit\u001b[0;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[1;32m   1046\u001b[0m evals_result: _EvalResultDict \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m   1047\u001b[0m callbacks\u001b[38;5;241m.\u001b[39mappend(record_evaluation(evals_result))\n\u001b[0;32m-> 1049\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1050\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1051\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_boost_round\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_estimators\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1053\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalid_sets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_sets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1054\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalid_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1055\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_metrics_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1056\u001b[0m \u001b[43m    \u001b[49m\u001b[43minit_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1057\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1058\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1060\u001b[0m \u001b[38;5;66;03m# This populates the property self.n_features_, the number of features in the fitted model,\u001b[39;00m\n\u001b[1;32m   1061\u001b[0m \u001b[38;5;66;03m# and so should only be set after fitting.\u001b[39;00m\n\u001b[1;32m   1062\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m   1063\u001b[0m \u001b[38;5;66;03m# The related property self._n_features_in, which populates self.n_features_in_,\u001b[39;00m\n\u001b[1;32m   1064\u001b[0m \u001b[38;5;66;03m# is set BEFORE fitting.\u001b[39;00m\n\u001b[1;32m   1065\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster\u001b[38;5;241m.\u001b[39mnum_feature()\n",
      "File \u001b[0;32m~/yangsuienv/lib/python3.10/site-packages/lightgbm/engine.py:322\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, feval, init_model, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cb \u001b[38;5;129;01min\u001b[39;00m callbacks_before_iter:\n\u001b[1;32m    311\u001b[0m     cb(\n\u001b[1;32m    312\u001b[0m         callback\u001b[38;5;241m.\u001b[39mCallbackEnv(\n\u001b[1;32m    313\u001b[0m             model\u001b[38;5;241m=\u001b[39mbooster,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    319\u001b[0m         )\n\u001b[1;32m    320\u001b[0m     )\n\u001b[0;32m--> 322\u001b[0m \u001b[43mbooster\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    324\u001b[0m evaluation_result_list: List[_LGBM_BoosterEvalMethodResultType] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    325\u001b[0m \u001b[38;5;66;03m# check evaluation result.\u001b[39;00m\n",
      "File \u001b[0;32m~/yangsuienv/lib/python3.10/site-packages/lightgbm/basic.py:4155\u001b[0m, in \u001b[0;36mBooster.update\u001b[0;34m(self, train_set, fobj)\u001b[0m\n\u001b[1;32m   4152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__set_objective_to_none:\n\u001b[1;32m   4153\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LightGBMError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot update due to null objective function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   4154\u001b[0m _safe_call(\n\u001b[0;32m-> 4155\u001b[0m     \u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLGBM_BoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4156\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mis_finished\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4158\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4159\u001b[0m )\n\u001b[1;32m   4160\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__is_predicted_cur_iter \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__num_dataset)]\n\u001b[1;32m   4161\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m is_finished\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import lightgbm as lgb\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# 配置\n",
    "# ==============================\n",
    "ZIP_PATH = Path(\"blogfeedback.zip\")      # 本地 BlogFeedback zip 路径\n",
    "CSV_NAME = \"blogData_train.csv\"         # zip 里的文件名\n",
    "\n",
    "PPPI_TEST_FRACTION = 0.3                # 留给 PPCI 的比例\n",
    "RANDOM_STATE = 2025\n",
    "N_X0 = 50                               # x0 个数（从整体干净数据中先抽出来）\n",
    "\n",
    "OUT_CSV_PPCI = \"blogfeedback_ppci.csv\"\n",
    "OUT_CSV_X0   = \"blogfeedback_ppci_x0.csv\"\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# 1. 从 zip 读取原始数据\n",
    "# ==============================\n",
    "if not ZIP_PATH.exists():\n",
    "    raise FileNotFoundError(f\"找不到 zip 文件: {ZIP_PATH}\")\n",
    "\n",
    "with zipfile.ZipFile(ZIP_PATH, \"r\") as zf:\n",
    "    print(\"Files in zip:\", zf.namelist())\n",
    "    if CSV_NAME not in zf.namelist():\n",
    "        raise FileNotFoundError(f\"zip 里找不到 {CSV_NAME}，请检查文件名。\")\n",
    "    with zf.open(CSV_NAME) as f:\n",
    "        df_raw = pd.read_csv(f, header=None)\n",
    "\n",
    "print(\"原始 df_raw 形状:\", df_raw.shape)  # 例如 (52397, 281)\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# 2. 去掉重复数据\n",
    "#    (1) 先去掉整行重复 (X+Y 都一样)\n",
    "#    (2) 再去掉 X 重复的所有行，只保留 X 只出现一次的样本\n",
    "# ==============================\n",
    "\n",
    "# (1) 去掉整行重复\n",
    "df_nodup_all = df_raw.drop_duplicates()\n",
    "print(\"去掉整行重复后行数:\", len(df_nodup_all))\n",
    "\n",
    "# (2) 去掉 X 重复的行\n",
    "X_all = df_nodup_all.iloc[:, :-1]   # 前 280 列是 X\n",
    "Y_all = df_nodup_all.iloc[:, -1]    # 最后一列是 Y\n",
    "\n",
    "# mask_uniqueX 为 True 的行是：X 在整个数据集中只出现一次\n",
    "mask_uniqueX = ~X_all.duplicated(keep=False)\n",
    "df_clean = df_nodup_all[mask_uniqueX].reset_index(drop=True)\n",
    "\n",
    "print(\"最终去掉 X 重复后行数:\", len(df_clean))\n",
    "\n",
    "# 提取干净的 X_raw, Y_raw\n",
    "X_clean_raw = df_clean.iloc[:, :-1].to_numpy(dtype=float)\n",
    "Y_clean_raw = df_clean.iloc[:, -1].to_numpy(dtype=float)\n",
    "\n",
    "print(\"X_clean_raw shape:\", X_clean_raw.shape)\n",
    "print(\"Y_clean_raw shape:\", Y_clean_raw.shape)\n",
    "print(\"Y_clean_raw min/max:\", Y_clean_raw.min(), Y_clean_raw.max())\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# 3. 对 X 做全局标准化，对 Y 做 log1p\n",
    "# ==============================\n",
    "X_mean = X_clean_raw.mean(axis=0)\n",
    "X_std = X_clean_raw.std(axis=0)\n",
    "X_std_safe = np.where(X_std == 0, 1.0, X_std)\n",
    "\n",
    "X_clean_std = (X_clean_raw - X_mean) / X_std_safe\n",
    "Y_log = np.log1p(Y_clean_raw)\n",
    "\n",
    "n_clean, d = X_clean_std.shape\n",
    "print(\"标准化后 X_clean_std 形状:\", X_clean_std.shape)\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# 4. 先从整体干净数据中抽 N_X0 个 x0\n",
    "#    然后再对剩下的样本做 train/test split\n",
    "# ==============================\n",
    "rng = np.random.default_rng(RANDOM_STATE)\n",
    "\n",
    "if N_X0 >= n_clean:\n",
    "    raise ValueError(\"N_X0 太大，没有足够样本留给训练和 PPCI。\")\n",
    "\n",
    "# 4.1 随机抽 N_X0 行作为 x0 的原始索引\n",
    "idx_all = np.arange(n_clean)\n",
    "idx_x0 = rng.choice(idx_all, size=N_X0, replace=False)\n",
    "\n",
    "# 4.2 剩余样本索引（用于 LightGBM 训练 + PPCI）\n",
    "mask_rem = np.ones(n_clean, dtype=bool)\n",
    "mask_rem[idx_x0] = False\n",
    "idx_rem = idx_all[mask_rem]\n",
    "\n",
    "X_rem = X_clean_std[idx_rem]\n",
    "Y_rem_log = Y_log[idx_rem]\n",
    "Y_rem_raw = Y_clean_raw[idx_rem]\n",
    "\n",
    "print(f\"用于后续 train/test split 的剩余样本数: {X_rem.shape[0]}\")\n",
    "\n",
    "# 4.3 在“剩余样本”上做 train/test split\n",
    "#     - 训练 LightGBM 的额外样本：X_train_extra\n",
    "#     - PPCI 的样本：X_ppci\n",
    "X_train_extra, X_ppci, Y_train_extra_log, Y_ppci_log, Y_train_extra_raw, Y_ppci_raw = train_test_split(\n",
    "    X_rem,\n",
    "    Y_rem_log,\n",
    "    Y_rem_raw,\n",
    "    test_size=PPPI_TEST_FRACTION,\n",
    "    random_state=RANDOM_STATE,\n",
    ")\n",
    "\n",
    "print(f\"LightGBM 额外训练样本数: {X_train_extra.shape[0]}\")\n",
    "print(f\"PPCI 样本数:             {X_ppci.shape[0]}\")\n",
    "\n",
    "# 4.4 LightGBM 训练集 = x0 样本 + 额外训练样本\n",
    "X_pred = np.vstack([X_clean_std[idx_x0], X_train_extra])\n",
    "Y_pred_log = np.concatenate([Y_log[idx_x0], Y_train_extra_log])\n",
    "Y_pred_raw = np.concatenate([Y_clean_raw[idx_x0], Y_train_extra_raw])\n",
    "\n",
    "print(f\"LightGBM 总训练样本数 (含 x0): {X_pred.shape[0]}\")\n",
    "\n",
    "# 4.5 x0 标准化后的坐标\n",
    "X_x0_std = X_clean_std[idx_x0, :]\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# 5. 训练 LightGBM 回归模型（在 log 标度上）\n",
    "# ==============================\n",
    "lgbm = lgb.LGBMRegressor(\n",
    "    objective=\"regression\",\n",
    "    boosting_type=\"gbdt\",\n",
    "    n_estimators=500,\n",
    "    learning_rate=0.05,\n",
    "    num_leaves=63,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_lambda=1.0,\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "X_tr, X_val, Y_tr, Y_val = train_test_split(\n",
    "    X_pred,\n",
    "    Y_pred_log,\n",
    "    test_size=0.2,\n",
    "    random_state=RANDOM_STATE,\n",
    ")\n",
    "\n",
    "print(\"Fitting LightGBM ...\")\n",
    "lgbm.fit(\n",
    "    X_tr,\n",
    "    Y_tr,\n",
    "    eval_set=[(X_val, Y_val)],\n",
    "    eval_metric=\"l2\",\n",
    ")\n",
    "\n",
    "# 在 PPCI 那部分上预测（log 标度）\n",
    "Yhat_ppci_log = lgbm.predict(X_ppci)\n",
    "\n",
    "# 看一下模型表现\n",
    "mse_log = mean_squared_error(Y_ppci_log, Yhat_ppci_log)\n",
    "r2_log = r2_score(Y_ppci_log, Yhat_ppci_log)\n",
    "print(f\"[Log scale]  MSE = {mse_log:.4f}, R^2 = {r2_log:.4f}\")\n",
    "\n",
    "Yhat_ppci_raw = np.expm1(Yhat_ppci_log)\n",
    "mse_raw = mean_squared_error(Y_ppci_raw, Yhat_ppci_raw)\n",
    "r2_raw = r2_score(Y_ppci_raw, Yhat_ppci_raw)\n",
    "print(f\"[Raw scale]  MSE = {mse_raw:.2f}, R^2 = {r2_raw:.4f}\")\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# 6. 生成 PPCI 用的 CSV\n",
    "#    列：x1,...,xD, logy, logyhat\n",
    "# ==============================\n",
    "cols_X = [f\"x{j+1}\" for j in range(d)]\n",
    "\n",
    "df_ppci = pd.DataFrame(X_ppci, columns=cols_X)\n",
    "df_ppci[\"logy\"] = Y_ppci_log\n",
    "df_ppci[\"logyhat\"] = Yhat_ppci_log\n",
    "\n",
    "df_ppci.to_csv(OUT_CSV_PPCI, index=False)\n",
    "print(f\"PPCI 数据已写入: {OUT_CSV_PPCI}\")\n",
    "print(df_ppci.head())\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# 7. 生成 x0 的 CSV（来自整体干净数据，且保证不在 PPCI 样本中）\n",
    "# ==============================\n",
    "df_x0 = pd.DataFrame(X_x0_std, columns=cols_X)\n",
    "df_x0.to_csv(OUT_CSV_X0, index=False)\n",
    "print(f\"{N_X0} 个 x0 已写入: {OUT_CSV_X0}\")\n",
    "print(df_x0.head())\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# 8. 检查 1：PPCI 里 X 是否有重复行\n",
    "# ==============================\n",
    "# 把 X_ppci 单独拿出来做 DataFrame，方便用 duplicated 检查\n",
    "X_ppci_df = pd.DataFrame(X_ppci, columns=cols_X)\n",
    "\n",
    "# duplicated(keep=False) 会把所有属于“重复组”的行都标 True\n",
    "dup_mask_ppci = X_ppci_df.duplicated(keep=False)\n",
    "n_dup_ppci = dup_mask_ppci.sum()\n",
    "n_unique_ppci = len(X_ppci_df.drop_duplicates())\n",
    "\n",
    "print(\"\\n[Check 1] PPCI 中 X 的重复情况：\")\n",
    "print(f\"  PPCI 总行数: {len(X_ppci_df)}\")\n",
    "print(f\"  去重后行数: {n_unique_ppci}\")\n",
    "print(f\"  属于重复 X 的行数: {n_dup_ppci}\")\n",
    "print(f\"  重复比例: {n_dup_ppci / len(X_ppci_df):.4f}\")\n",
    "\n",
    "if n_dup_ppci > 0:\n",
    "    print(\"  示例重复的几行 X：\")\n",
    "    print(X_ppci_df[dup_mask_ppci].head())\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# 9. 检查 2：x0 是否和 PPCI 中的 X 有重复\n",
    "# ==============================\n",
    "matches = 0\n",
    "match_detail = []\n",
    "\n",
    "for i in range(N_X0):\n",
    "    # 对每一个 x0，看在 X_ppci 里是否存在完全相同的一行\n",
    "    # 用 isclose 做逐维比较，避免浮点误差\n",
    "    same_rows = np.all(np.isclose(X_ppci, X_x0_std[i], atol=1e-12), axis=1)\n",
    "    if np.any(same_rows):\n",
    "        matches += 1\n",
    "        idxs = np.where(same_rows)[0]\n",
    "        match_detail.append((i, idxs[:5]))  # 只记录前几个位置，避免太长\n",
    "\n",
    "print(\"\\n[Check 2] x0 与 PPCI 中 X 的交集情况：\")\n",
    "print(f\"  x0 的个数: {N_X0}\")\n",
    "print(f\"  在 PPCI 中能找到完全相同 X 的 x0 个数: {matches}\")\n",
    "\n",
    "if matches > 0:\n",
    "    print(\"  其中一些匹配示例 (x0_index -> PPCI 行索引的前几个):\")\n",
    "    for x0_idx, p_idx in match_detail:\n",
    "        print(f\"    x0[{x0_idx}] -> PPCI rows {p_idx}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69399d42-3da2-4330-a246-d00fb1333cbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in zip: ['blogData_test-2012.02.01.00_00.csv', 'blogData_test-2012.02.02.00_00.csv', 'blogData_test-2012.02.03.00_00.csv', 'blogData_test-2012.02.04.00_00.csv', 'blogData_test-2012.02.05.00_00.csv', 'blogData_test-2012.02.06.00_00.csv', 'blogData_test-2012.02.07.00_00.csv', 'blogData_test-2012.02.08.00_00.csv', 'blogData_test-2012.02.09.00_00.csv', 'blogData_test-2012.02.10.00_00.csv', 'blogData_test-2012.02.11.00_00.csv', 'blogData_test-2012.02.12.00_00.csv', 'blogData_test-2012.02.13.00_00.csv', 'blogData_test-2012.02.14.00_00.csv', 'blogData_test-2012.02.15.00_00.csv', 'blogData_test-2012.02.16.00_00.csv', 'blogData_test-2012.02.17.00_00.csv', 'blogData_test-2012.02.18.00_00.csv', 'blogData_test-2012.02.19.00_00.csv', 'blogData_test-2012.02.20.00_00.csv', 'blogData_test-2012.02.21.00_00.csv', 'blogData_test-2012.02.22.00_00.csv', 'blogData_test-2012.02.23.00_00.csv', 'blogData_test-2012.02.24.00_00.csv', 'blogData_test-2012.02.25.00_00.csv', 'blogData_test-2012.02.26.00_00.csv', 'blogData_test-2012.02.27.00_00.csv', 'blogData_test-2012.02.28.00_00.csv', 'blogData_test-2012.02.29.00_00.csv', 'blogData_test-2012.03.01.00_00.csv', 'blogData_test-2012.03.02.00_00.csv', 'blogData_test-2012.03.03.00_00.csv', 'blogData_test-2012.03.04.00_00.csv', 'blogData_test-2012.03.05.00_00.csv', 'blogData_test-2012.03.06.00_00.csv', 'blogData_test-2012.03.07.00_00.csv', 'blogData_test-2012.03.08.00_00.csv', 'blogData_test-2012.03.09.00_00.csv', 'blogData_test-2012.03.10.00_00.csv', 'blogData_test-2012.03.11.00_00.csv', 'blogData_test-2012.03.12.00_00.csv', 'blogData_test-2012.03.13.00_00.csv', 'blogData_test-2012.03.14.00_00.csv', 'blogData_test-2012.03.15.00_00.csv', 'blogData_test-2012.03.16.00_00.csv', 'blogData_test-2012.03.17.00_00.csv', 'blogData_test-2012.03.18.00_00.csv', 'blogData_test-2012.03.19.00_00.csv', 'blogData_test-2012.03.20.00_00.csv', 'blogData_test-2012.03.21.00_00.csv', 'blogData_test-2012.03.22.00_00.csv', 'blogData_test-2012.03.23.00_00.csv', 'blogData_test-2012.03.24.00_00.csv', 'blogData_test-2012.03.25.00_00.csv', 'blogData_test-2012.03.26.01_00.csv', 'blogData_test-2012.03.27.01_00.csv', 'blogData_test-2012.03.28.01_00.csv', 'blogData_test-2012.03.29.01_00.csv', 'blogData_test-2012.03.30.01_00.csv', 'blogData_test-2012.03.31.01_00.csv', 'blogData_train.csv']\n",
      "Raw df_raw shape: (52397, 281)\n",
      "Rows after dropping full-row duplicates: 49203\n",
      "Final rows after dropping duplicated X: 48439\n",
      "X_clean_raw shape: (48439, 280)\n",
      "Y_clean_raw shape: (48439,)\n",
      "Y_clean_raw min/max: 0.0 1370.0\n",
      "Standardized X_clean_std shape: (48439, 280)\n",
      "Remaining samples for train/test split: 48389\n",
      "Extra training samples for LightGBM: 33872\n",
      "PPCI samples:                      14517\n",
      "Total LightGBM training samples (including x0): 33922\n",
      "Fitting LightGBM ...\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 1.889718 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8503\n",
      "[LightGBM] [Info] Number of data points in the train set: 27137, number of used features: 212\n",
      "[LightGBM] [Info] Start training from score 0.608789\n",
      "[Log scale]  MSE = 0.4339, R^2 = 0.6634\n",
      "[Raw scale]  MSE = 739.39, R^2 = 0.5484\n",
      "PPCI data written to: blogfeedback_ppci.csv\n",
      "         x1        x2       x3        x4        x5        x6        x7  \\\n",
      "0 -0.434195 -0.476406 -0.05106 -0.374994 -0.339400 -0.408048 -0.449455   \n",
      "1 -0.474423 -0.719423 -0.05106 -0.733278 -0.339400 -0.450513 -0.695617   \n",
      "2 -0.473733 -0.715007 -0.05106 -0.728772 -0.339400 -0.449764 -0.690760   \n",
      "3 -0.314662 -0.350751 -0.05106 -0.354713 -0.283559 -0.299974 -0.351213   \n",
      "4 -0.474423 -0.719423 -0.05106 -0.733278 -0.339400 -0.450513 -0.695617   \n",
      "\n",
      "         x8        x9       x10  ...      x273      x274      x275      x276  \\\n",
      "0 -0.021532 -0.326176 -0.235381  ... -0.439708 -0.428761 -0.326993 -0.318846   \n",
      "1 -0.021532 -0.763367 -0.235381  ... -0.439708 -0.428761 -0.326993  3.136314   \n",
      "2 -0.021532 -0.757295 -0.235381  ... -0.439708 -0.428761 -0.326993 -0.318846   \n",
      "3 -0.021532 -0.332248 -0.235381  ... -0.439708 -0.428761 -0.326993 -0.318846   \n",
      "4 -0.021532 -0.763367 -0.235381  ... -0.439708  2.332304 -0.326993 -0.318846   \n",
      "\n",
      "       x277  x278      x279      x280  logy   logyhat  \n",
      "0 -0.086203   0.0 -0.046984 -0.039354   0.0 -0.023977  \n",
      "1 -0.086203   0.0 -0.046984 -0.039354   0.0  0.001468  \n",
      "2 -0.086203   0.0 -0.046984 -0.039354   0.0  0.043997  \n",
      "3 -0.086203   0.0 -0.046984 -0.039354   0.0  0.068185  \n",
      "4 -0.086203   0.0 -0.046984 -0.039354   0.0  0.001130  \n",
      "\n",
      "[5 rows x 282 columns]\n",
      "50 x0 points written to: blogfeedback_ppci_x0.csv\n",
      "         x1        x2        x3        x4        x5        x6        x7  \\\n",
      "0 -0.474166 -0.717159 -0.051060 -0.731025 -0.339400 -0.450199 -0.693041   \n",
      "1  1.053391  1.334181 -0.051060  1.666554  0.875132  0.855145  0.897033   \n",
      "2 -0.471007 -0.706529 -0.051060 -0.724265 -0.339400 -0.447268 -0.684604   \n",
      "3 -0.343303 -0.435981  0.089532 -0.149657 -0.269599 -0.329428 -0.431839   \n",
      "4 -0.473733 -0.715007 -0.051060 -0.728772 -0.339400 -0.449764 -0.690760   \n",
      "\n",
      "         x8        x9       x10  ...      x271      x272      x273      x274  \\\n",
      "0 -0.021532 -0.760331 -0.235381  ...  2.250327 -0.452582 -0.439708 -0.428761   \n",
      "1 -0.021532  0.727333  0.557072  ... -0.444380 -0.452582 -0.439708 -0.428761   \n",
      "2 -0.021532 -0.754259 -0.235381  ... -0.444380 -0.452582 -0.439708 -0.428761   \n",
      "3 -0.021532 -0.049896 -0.194743  ... -0.444380 -0.452582  2.274235 -0.428761   \n",
      "4 -0.021532 -0.757295 -0.235381  ... -0.444380  2.209545 -0.439708 -0.428761   \n",
      "\n",
      "       x275      x276      x277  x278      x279      x280  \n",
      "0 -0.326993 -0.318846 -0.086203   0.0 -0.046984 -0.039354  \n",
      "1 -0.326993 -0.318846 -0.086203   0.0 -0.046984 -0.039354  \n",
      "2 -0.326993  3.136314 -0.086203   0.0 -0.046984 -0.039354  \n",
      "3 -0.326993 -0.318846 -0.086203   0.0 -0.046984 -0.039354  \n",
      "4 -0.326993 -0.318846 -0.086203   0.0 -0.046984 -0.039354  \n",
      "\n",
      "[5 rows x 280 columns]\n",
      "\n",
      "[Check 1] Duplicates in PPCI X:\n",
      "  PPCI total rows:    14517\n",
      "  Rows after dedup:   14517\n",
      "  Rows in duplicate groups: 0\n",
      "  Duplicate ratio:    0.0000\n",
      "\n",
      "[Check 2 - FAST] Intersection between x0 and PPCI X:\n",
      "  Number of x0 points: 50\n",
      "  Number of x0 rows found in PPCI X: 0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import lightgbm as lgb\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# Config\n",
    "# ==============================\n",
    "ZIP_PATH = Path(\"blogfeedback.zip\")      # Local path to BlogFeedback zip\n",
    "CSV_NAME = \"blogData_train.csv\"         # CSV name inside the zip\n",
    "\n",
    "PPPI_TEST_FRACTION = 0.3                # Fraction reserved for PPCI\n",
    "RANDOM_STATE = 2025\n",
    "N_X0 = 50                               # Number of x0 points sampled from the clean pool\n",
    "\n",
    "OUT_CSV_PPCI = \"blogfeedback_ppci.csv\"\n",
    "OUT_CSV_X0   = \"blogfeedback_ppci_x0.csv\"\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# 1. Read raw data from zip\n",
    "# ==============================\n",
    "if not ZIP_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Zip file not found: {ZIP_PATH}\")\n",
    "\n",
    "with zipfile.ZipFile(ZIP_PATH, \"r\") as zf:\n",
    "    print(\"Files in zip:\", zf.namelist())\n",
    "    if CSV_NAME not in zf.namelist():\n",
    "        raise FileNotFoundError(f\"{CSV_NAME} not found in zip. Please check the filename.\")\n",
    "    with zf.open(CSV_NAME) as f:\n",
    "        df_raw = pd.read_csv(f, header=None)\n",
    "\n",
    "print(\"Raw df_raw shape:\", df_raw.shape)  # e.g. (52397, 281)\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# 2. Remove duplicates\n",
    "#    (1) Drop full-row duplicates (X+Y identical)\n",
    "#    (2) Drop any rows whose X appears more than once (keep only unique X)\n",
    "# ==============================\n",
    "\n",
    "# (1) Drop exact row duplicates\n",
    "df_nodup_all = df_raw.drop_duplicates()\n",
    "print(\"Rows after dropping full-row duplicates:\", len(df_nodup_all))\n",
    "\n",
    "# (2) Drop duplicated X rows (keep only X that appears once in the dataset)\n",
    "X_all = df_nodup_all.iloc[:, :-1]   # first 280 columns are X\n",
    "Y_all = df_nodup_all.iloc[:, -1]    # last column is Y\n",
    "\n",
    "# mask_uniqueX == True means: X appears exactly once in the whole dataset\n",
    "mask_uniqueX = ~X_all.duplicated(keep=False)\n",
    "df_clean = df_nodup_all[mask_uniqueX].reset_index(drop=True)\n",
    "\n",
    "print(\"Final rows after dropping duplicated X:\", len(df_clean))\n",
    "\n",
    "# Extract clean X_raw, Y_raw\n",
    "X_clean_raw = df_clean.iloc[:, :-1].to_numpy(dtype=float)\n",
    "Y_clean_raw = df_clean.iloc[:, -1].to_numpy(dtype=float)\n",
    "\n",
    "print(\"X_clean_raw shape:\", X_clean_raw.shape)\n",
    "print(\"Y_clean_raw shape:\", Y_clean_raw.shape)\n",
    "print(\"Y_clean_raw min/max:\", Y_clean_raw.min(), Y_clean_raw.max())\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# 3. Global standardization for X, log1p for Y\n",
    "# ==============================\n",
    "X_mean = X_clean_raw.mean(axis=0)\n",
    "X_std = X_clean_raw.std(axis=0)\n",
    "X_std_safe = np.where(X_std == 0, 1.0, X_std)\n",
    "\n",
    "X_clean_std = (X_clean_raw - X_mean) / X_std_safe\n",
    "Y_log = np.log1p(Y_clean_raw)\n",
    "\n",
    "n_clean, d = X_clean_std.shape\n",
    "print(\"Standardized X_clean_std shape:\", X_clean_std.shape)\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# 4. Sample N_X0 x0 points from the clean pool,\n",
    "#    then split the remaining samples into train/test (extra train vs PPCI).\n",
    "# ==============================\n",
    "rng = np.random.default_rng(RANDOM_STATE)\n",
    "\n",
    "if N_X0 >= n_clean:\n",
    "    raise ValueError(\"N_X0 is too large; not enough samples left for training and PPCI.\")\n",
    "\n",
    "# 4.1 Randomly sample N_X0 indices as x0\n",
    "idx_all = np.arange(n_clean)\n",
    "idx_x0 = rng.choice(idx_all, size=N_X0, replace=False)\n",
    "\n",
    "# 4.2 Remaining indices (used for LightGBM training + PPCI)\n",
    "mask_rem = np.ones(n_clean, dtype=bool)\n",
    "mask_rem[idx_x0] = False\n",
    "idx_rem = idx_all[mask_rem]\n",
    "\n",
    "X_rem = X_clean_std[idx_rem]\n",
    "Y_rem_log = Y_log[idx_rem]\n",
    "Y_rem_raw = Y_clean_raw[idx_rem]\n",
    "\n",
    "print(f\"Remaining samples for train/test split: {X_rem.shape[0]}\")\n",
    "\n",
    "# 4.3 Train/test split on the remaining samples\n",
    "#     - X_train_extra: additional samples for LightGBM training\n",
    "#     - X_ppci: samples reserved for PPCI\n",
    "X_train_extra, X_ppci, Y_train_extra_log, Y_ppci_log, Y_train_extra_raw, Y_ppci_raw = train_test_split(\n",
    "    X_rem,\n",
    "    Y_rem_log,\n",
    "    Y_rem_raw,\n",
    "    test_size=PPPI_TEST_FRACTION,\n",
    "    random_state=RANDOM_STATE,\n",
    ")\n",
    "\n",
    "print(f\"Extra training samples for LightGBM: {X_train_extra.shape[0]}\")\n",
    "print(f\"PPCI samples:                      {X_ppci.shape[0]}\")\n",
    "\n",
    "# 4.4 LightGBM training set = x0 samples + extra training samples\n",
    "X_pred = np.vstack([X_clean_std[idx_x0], X_train_extra])\n",
    "Y_pred_log = np.concatenate([Y_log[idx_x0], Y_train_extra_log])\n",
    "Y_pred_raw = np.concatenate([Y_clean_raw[idx_x0], Y_train_extra_raw])\n",
    "\n",
    "print(f\"Total LightGBM training samples (including x0): {X_pred.shape[0]}\")\n",
    "\n",
    "# 4.5 x0 standardized coordinates\n",
    "X_x0_std = X_clean_std[idx_x0, :]\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# 5. Train LightGBM regressor (on log scale)\n",
    "# ==============================\n",
    "lgbm = lgb.LGBMRegressor(\n",
    "    objective=\"regression\",\n",
    "    boosting_type=\"gbdt\",\n",
    "    n_estimators=500,\n",
    "    learning_rate=0.05,\n",
    "    num_leaves=63,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_lambda=1.0,\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "X_tr, X_val, Y_tr, Y_val = train_test_split(\n",
    "    X_pred,\n",
    "    Y_pred_log,\n",
    "    test_size=0.2,\n",
    "    random_state=RANDOM_STATE,\n",
    ")\n",
    "\n",
    "print(\"Fitting LightGBM ...\")\n",
    "lgbm.fit(\n",
    "    X_tr,\n",
    "    Y_tr,\n",
    "    eval_set=[(X_val, Y_val)],\n",
    "    eval_metric=\"l2\",\n",
    ")\n",
    "\n",
    "# Predict on PPCI subset (log scale)\n",
    "Yhat_ppci_log = lgbm.predict(X_ppci)\n",
    "\n",
    "# Basic performance checks\n",
    "mse_log = mean_squared_error(Y_ppci_log, Yhat_ppci_log)\n",
    "r2_log = r2_score(Y_ppci_log, Yhat_ppci_log)\n",
    "print(f\"[Log scale]  MSE = {mse_log:.4f}, R^2 = {r2_log:.4f}\")\n",
    "\n",
    "Yhat_ppci_raw = np.expm1(Yhat_ppci_log)\n",
    "mse_raw = mean_squared_error(Y_ppci_raw, Yhat_ppci_raw)\n",
    "r2_raw = r2_score(Y_ppci_raw, Yhat_ppci_raw)\n",
    "print(f\"[Raw scale]  MSE = {mse_raw:.2f}, R^2 = {r2_raw:.4f}\")\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# 6. Write PPCI CSV\n",
    "#    Columns: x1,...,xD, logy, logyhat\n",
    "# ==============================\n",
    "cols_X = [f\"x{j+1}\" for j in range(d)]\n",
    "\n",
    "df_ppci = pd.DataFrame(X_ppci, columns=cols_X)\n",
    "df_ppci[\"logy\"] = Y_ppci_log\n",
    "df_ppci[\"logyhat\"] = Yhat_ppci_log\n",
    "\n",
    "df_ppci.to_csv(OUT_CSV_PPCI, index=False)\n",
    "print(f\"PPCI data written to: {OUT_CSV_PPCI}\")\n",
    "print(df_ppci.head())\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# 7. Write x0 CSV (sampled from the clean pool and excluded from PPCI by construction)\n",
    "# ==============================\n",
    "df_x0 = pd.DataFrame(X_x0_std, columns=cols_X)\n",
    "df_x0.to_csv(OUT_CSV_X0, index=False)\n",
    "print(f\"{N_X0} x0 points written to: {OUT_CSV_X0}\")\n",
    "print(df_x0.head())\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# 8. Check 1: whether X in PPCI has duplicate rows\n",
    "# ==============================\n",
    "X_ppci_df = pd.DataFrame(X_ppci, columns=cols_X)\n",
    "\n",
    "# duplicated(keep=False) marks all rows that are part of any duplicate group\n",
    "dup_mask_ppci = X_ppci_df.duplicated(keep=False)\n",
    "n_dup_ppci = int(dup_mask_ppci.sum())\n",
    "n_unique_ppci = len(X_ppci_df.drop_duplicates())\n",
    "\n",
    "print(\"\\n[Check 1] Duplicates in PPCI X:\")\n",
    "print(f\"  PPCI total rows:    {len(X_ppci_df)}\")\n",
    "print(f\"  Rows after dedup:   {n_unique_ppci}\")\n",
    "print(f\"  Rows in duplicate groups: {n_dup_ppci}\")\n",
    "print(f\"  Duplicate ratio:    {n_dup_ppci / len(X_ppci_df):.4f}\")\n",
    "\n",
    "if n_dup_ppci > 0:\n",
    "    print(\"  Example duplicate X rows:\")\n",
    "    print(X_ppci_df[dup_mask_ppci].head())\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# 9. Check 2 (FAST): whether any x0 row appears in PPCI X\n",
    "#    This replaces the slow O(N_X0 * n_ppci * d) loop with a vectorized row-match.\n",
    "#    Since X_ppci and X_x0_std are slices from the same standardized array,\n",
    "#    exact row matching is safe here.\n",
    "# ==============================\n",
    "X_ppci_c = np.ascontiguousarray(X_ppci)\n",
    "X_x0_c = np.ascontiguousarray(X_x0_std)\n",
    "\n",
    "# View each row as a single \"byte string\" (void) so we can do fast set-like membership tests.\n",
    "row_dtype = np.dtype((np.void, X_ppci_c.dtype.itemsize * X_ppci_c.shape[1]))\n",
    "ppci_rows = X_ppci_c.view(row_dtype).ravel()\n",
    "x0_rows = X_x0_c.view(row_dtype).ravel()\n",
    "\n",
    "matches = int(np.isin(x0_rows, ppci_rows).sum())\n",
    "\n",
    "print(\"\\n[Check 2 - FAST] Intersection between x0 and PPCI X:\")\n",
    "print(f\"  Number of x0 points: {N_X0}\")\n",
    "print(f\"  Number of x0 rows found in PPCI X: {matches}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ad5d24-8444-469e-b178-5d42f993f9b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
